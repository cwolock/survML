---
title: "Assessing variable importance in survival analysis using machine learning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assessing variable importance in survival analysis using machine learning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(survML)
library(survival)
library(dplyr)
library(ggplot2)
set.seed(72924)
```

## Introduction

The `survML` package includes functions that can be used to estimate model-free, algorithm-agnostic variable importance when the outcome of interest is subject to right censoring. Specifically, this functionality is aimed at estimating *intrinsic* variable importance, which is the population-level predictiveness potential of a feature or group of features.

Suppose we have access to a vector $X$ of features, which we wish to use to make a prediction involving $T$, a time-to-event outcome. We use $C$ to denote the right censoring variable. The observed data are given by $(X, Y, \Delta)$, where $\Delta := I(T \leq C)$ and $Y := \text{min}(T, C)$. For an index set $s$, we use $X_s$ to denote the elements of $X$ with index in $s$ and $X_{-s}$ its complement. For a given prediction task (say, estimating the probability that $T$ is smaller than some landmark time $\tau$) and prediction function $f$, we require a measure of predictiveness. We let $V(f, P_0)$ denote the predictiveness of $f$ under sampling from distribution $P_0$. We define $f_{0,s}$ as the *oracle prediction function* excluding features with index in $s$; this is the best possible prediction function, according to $V$, that uses only $X_{-s}$. 

For intrinsic variable importance, we consider nested index sets $r \subset s$ and define the importance of $X_{s \setminus r}$ relative to $X_s$ as $V(f_{0,r}, P_0) - V(f_{0,s}, P_0)$; this is the difference in maximum achievable predictiveness when only $r$ is excluded compared to when $s$ is excluded. We refer to this parameter as a variable importance measure (VIM).

Due to right censoring, the VIM estimation procedure requires estimates of the conditional survival functions of $T$ and $C$ given $X$, which we define pointwise as $S_0(t \mid x) := P_0(T > t \mid X = x)$ and $G_0(t \mid x) := P_0(C > t \mid X = x)$, respectively. These functions must be estimated over the interval $(0, \tau]$ and may be obtained from any conditional survival estimation algorithm. This may be as simple as a Cox proportional hazards model (Cox, 1972) or parametric survival regression model, or as complex as a stacked regression procedure such as survival Super Learner (Westling et al., 2023) or global survival stacking (Wolock et al., 2024).

We also require estimates of the oracle prediction functions $f_{0,r}$ and $f_{0,s}$, whose exact form depends on the chosen predictiveness measure. For several commonly used measures, the oracle prediction functions can be written in terms of $S_0(\cdot \mid x)$. The form of the oracle prediction function for the measures included in `survML` is given in the Appendix.

## Example: Predicting recurrence-free survival time in cancer patients

As an example, we consider estimating variable importance for predicting recurrence-free survival using the `gbsg` dataset in the `survival` package. The Kaplan-Meier estimate of the survival curve for this dataset is shown below. 

```{r km, fig.width=8, fig.height=6}
data(cancer)
km_fit <- survfit(Surv(rfstime, status) ~ 1, data = gbsg)
plot(km_fit, xlab = "Time (days)", ylab = "Recurrence-free survival probability")
```

We will consider time-varying AUC importance using landmark times of 500, 1000, 1500 and 2000 days. The first step is to prepare the data. We use dummy coding for factors. This means that to assess the importance of tumor grade, for example, which has three levels, we create two dummy variables called `tumgrad2` and `tumgrad3` and consider them as a single feature group. We also consider the feature groups defined by tumor-level features and patient-level features. 


```{r data_setup}
### variables of interest
# rfstime - recurrence-free survival
# status - censoring indicator
# hormon - hormonal therapy treatment indicator
# age - in years
# meno - 1 = premenopause, 2 = post
# size - tumor size in mm
# grade - factor 1,2,3
# nodes - number of positive nodes
# pgr - progesterone receptor in fmol
# er - estrogen receptor in fmol

# create dummy variables and clean data
gbsg$tumgrad2 <- ifelse(gbsg$grade == 2, 1, 0)
gbsg$tumgrad3 <- ifelse(gbsg$grade == 3, 1, 0)
gbsg <- gbsg %>% na.omit() %>% select(-c(pid, grade))

time <- gbsg$rfstime
event <- gbsg$status
X <- gbsg %>% select(-c(rfstime, status)) # remove outcome 

# find column indices of features/feature groups
X_names <- names(X)
age_index <- paste0(which(X_names == "age"))
meno_index <- paste0(which(X_names == "meno"))
size_index <- paste0(which(X_names == "size"))
nodes_index <- paste0(which(X_names == "nodes"))
pgr_index <- paste0(which(X_names == "pgr"))
er_index <- paste0(which(X_names == "er"))
hormon_index <- paste0(which(X_names == "hormon"))
grade_index <- paste0(which(X_names %in% c("tumgrad2", "tumgrad3")), collapse = ",")
tum_index <- paste0(which(X_names %in% c("size", "nodes", "pgr", "er", "tumgrad2", "tumgrad3")),
                    collapse = ",")
person_index <- paste0(which(X_names %in% c("age", "meno", "hormon")), collapse = ",")

feature_group_names <- c("age", "meno.", "size", "nodes",
                         "prog.", "estro.", "hormone", 
                         "grade")
feature_groups <- c(age_index, meno_index, size_index, nodes_index,
                    pgr_index, er_index, hormon_index, grade_index)

# consider joint importance of all tumor-level and person-level features
feature_group_names2 <- c("tumor", "person")
feature_groups2 <- c(tum_index, person_index)
```

Next, we write some functions to estimate all the relevant nuisance parameters. 

```{r}
# estimate conditional survival functions
generate_full_predictions <- function(time, 
                                      event, 
                                      X, 
                                      X_holdout, 
                                      landmark_times, 
                                      approx_times,
                                      SL.library){
  
    surv_out <- survML::stackG(time = time,
                               event = event,
                               X = X,
                               newX = rbind(X_holdout, X),
                               newtimes = approx_times,
                               time_grid_approx = approx_times,
                               bin_size = 0.05,
                               time_basis = "continuous",
                               surv_form = "PI",
                               SL_control = list(SL.library = SL.library,
                                                 V = 5))
    S_hat <- surv_out$S_T_preds[1:nrow(X_holdout),]
    G_hat <- surv_out$S_C_preds[1:nrow(X_holdout),]
    f_hat <- S_hat[,which(approx_times %in% landmark_times),drop=FALSE]
    S_hat_train <- surv_out$S_T_preds[(nrow(X_holdout)+1):(nrow(X_holdout)+nrow(X)),]
    G_hat_train <- surv_out$S_C_preds[(nrow(X_holdout)+1):(nrow(X_holdout)+nrow(X)),]
    f_hat_train <- S_hat_train[,which(approx_times %in% landmark_times),drop=FALSE]
  return(list(S_hat = S_hat,
              G_hat = G_hat,
              f_hat = f_hat,
              f_hat_train = f_hat_train,
              S_hat_train = S_hat_train,
              G_hat_train = G_hat_train))
}

# estimate residual oracle prediction function by regressing full predictions on reduced feature vector
generate_reduced_predictions <- function(f_hat,
                                         X_reduced,
                                         X_reduced_holdout,
                                         landmark_times,
                                         SL.library){
  
  long_dat <- data.frame(f_hat = f_hat, X_reduced)
  long_new_dat <- data.frame(X_reduced_holdout)
  reduced_fit <- SuperLearner::SuperLearner(Y = long_dat$f_hat,
                                            X = long_dat[,2:ncol(long_dat),drop=FALSE],
                                            family = stats::gaussian(),
                                            SL.library = SL.library,
                                            method = "method.NNLS",
                                            verbose = FALSE)
  fs_hat <- matrix(predict(reduced_fit, newdata = long_new_dat)$pred,
                   nrow = nrow(X_reduced_holdout),
                   ncol = length(landmark_times))
  
  return(list(fs_hat = fs_hat))
}

# produce cross-fitted full predictions
CV_generate_full_predictions <- function(time,
                                         event,
                                         X,
                                         landmark_times,
                                         approx_times,
                                         folds,
                                         sample_split,
                                         SL.library,
                                         DR_pred){
  .V <- length(unique(folds))
  CV_full_preds_train <- list()
  CV_full_preds <- list()
  CV_S_preds <- list()
  CV_S_preds_train <- list()
  CV_G_preds <- list()
  
  for (j in 1:.V){
    
    time_train <- time[folds != j]
    event_train <- event[folds != j]
    X_train <- X[folds != j,]
    
    X_holdout <- X[folds == j,]
    full_preds <- generate_full_predictions(time = time_train,
                                            event = event_train,
                                            X = X_train,
                                            X_holdout = X_holdout,
                                            landmark_times = landmark_times,
                                            approx_times = approx_times,
                                            SL.library = SL.library)
    if (!DR_pred){
        CV_full_preds_train[[j]] <- full_preds$f_hat_train
        CV_full_preds[[j]] <- full_preds$f_hat
      } else{ # doubly-robust version --- see below
        DR_preds <- generate_DR_predictions(
          time = time_train,
          event = event_train,
          X = X_train,
          X_holdout = X_holdout,
          landmark_times = landmark_times,
          approx_times = approx_times,
          S_hat = full_preds$S_hat_train,
          G_hat = full_preds$G_hat_train,
          SL.library = SL.library
        )
        CV_full_preds_train[[j]] <- DR_preds$f_hat_train
        CV_full_preds[[j]] <- DR_preds$f_hat
      }
    CV_S_preds[[j]] <- full_preds$S_hat
    CV_G_preds[[j]] <- full_preds$G_hat
    CV_S_preds_train[[j]] <- full_preds$S_hat_train
  }

  return(list(CV_full_preds_train = CV_full_preds_train,
              CV_full_preds = CV_full_preds,
              CV_S_preds = CV_S_preds,
              CV_S_preds_train = CV_S_preds_train,
              CV_G_preds = CV_G_preds))
}

# produce cross-fitted reduced predictions
CV_generate_reduced_predictions <- function(time,
                                            event,
                                            X,
                                            landmark_times,
                                            folds,
                                            indx,
                                            sample_split,
                                            full_preds_train,
                                            SL.library){
  .V <- length(unique(folds))
  CV_reduced_preds <- list()
  
  for (j in 1:.V){
    
    X_reduced_train <- X[folds != j,-indx,drop=FALSE]
    X_reduced_holdout <- X[folds == j,-indx,drop=FALSE]
    
    preds_j <- matrix(NA, nrow = nrow(X_reduced_holdout), ncol = length(landmark_times))
    for (t in landmark_times){
      outcomes <- full_preds_train[[j]][,which(landmark_times == t)]
      reduced_preds <- generate_reduced_predictions(f_hat = outcomes,
                                                    X_reduced = X_reduced_train,
                                                    X_reduced_holdout = X_reduced_holdout,
                                                    landmark_times = t,
                                                    SL.library = SL.library)
      
      preds_j[,which(landmark_times == t)] <- reduced_preds$fs_hat
    }
    
    CV_reduced_preds[[j]] <- preds_j
  }
  
  return(CV_reduced_preds)
}
```

### Estimating variable importance relative to all features

First, we consider the importance of each of the feature groups relative to the full feature vector. Here, the features of interest are subtracted from the full feature vector, with importance measured by the resulting loss in predictiveness. 

Note that because censoring may be informed by covariates that are not part of the current reduced feature set, we estimate the residual oracle prediction function by regressing the full oracle predictions on the reduced feature set, rather than directly estimating the conditional survival function using the reduced feature set. 

To reduce runtime, we use a very small Super Learner library. In actual analyses, it is generally a good idea to use a larger library of learners. 

```{r, fig.width=8, fig.height=8}
# super learner library for global survival stacking
SL.library <- c("SL.mean", "SL.glm")

# landmark times for AUC
landmark_times <- c(500, 1000, 1500, 2000)

# set up cross-fitting and sample-splitting folds
cf_fold_num <- 2
ss_fold_num <- 2*cf_fold_num
V <- ss_fold_num
folds <- sample(rep(seq_len(V), length = nrow(gbsg))) # 2V of them
ss_folds <- c(rep(1, V/2), rep(2, V/2))
ss_folds <- as.numeric(folds %in% which(ss_folds == 2))

# approximation time grid for integrals 
approx_times <- sort(unique(c(time[event == 1 & time <= max(landmark_times)], 
                              landmark_times)))

# generate nuisance estimates
V0_preds <- CV_generate_full_predictions(time = time,
                                         event = event,
                                         X = X,
                                         landmark_times = landmark_times,
                                         approx_times = approx_times,
                                         folds = folds,
                                         sample_split = TRUE,
                                         SL.library = SL.library,
                                         DR_pred = FALSE)

CV_full_preds <- V0_preds$CV_full_preds
CV_full_preds_train <- V0_preds$CV_full_preds_train
CV_S_preds <- V0_preds$CV_S_preds
CV_S_preds_train <- V0_preds$CV_S_preds_train
CV_G_preds <- V0_preds$CV_G_preds

# iterate over feature groups
for (i in 1:length(feature_group_names)){
  indx_char <- feature_groups[i]
  indx_name <- feature_group_names[i]
  indx <- as.numeric(strsplit(indx_char, split = ",")[[1]])

  # estimate residual oracle prediction function for this feature group
  CV_reduced_preds <- CV_generate_reduced_predictions(time = time,
                                              event = event,
                                              X = X,
                                              landmark_times = landmark_times,
                                              folds = folds,
                                              sample_split = sample_split,
                                              indx = indx,
                                              full_preds_train = CV_full_preds_train,
                                              SL.library = SL.library)
  # estimate VIM - note the oracle for AUC is the conditional cdf, not survival function, 
  # so need to take 1 - S(tau | x)
  output <- vim_AUC(time = time,
                    event = event,
                    approx_times = approx_times,
                    landmark_times = landmark_times,
                    f_hat = lapply(CV_full_preds, function(x) 1-x),
                    fs_hat = lapply(CV_reduced_preds, function(x) 1-x),
                    S_hat = CV_S_preds,
                    G_hat = CV_G_preds,
                    folds = folds,
                    ss_folds = ss_folds,
                    sample_split = TRUE,
                    scale_est = TRUE)
  
  output$vim <- "AUC"
  output$indx <- rep(indx_char, nrow(output))
  output$indx_name <- rep(indx_name, nrow(output))
  if (!(i == 1)){
    pooled_output <- rbind(pooled_output, output)
  } else{
    pooled_output <- output
  }
}

# plot results
p_auc <- pooled_output %>%
   mutate(landmark_time = factor(landmark_time,
                                levels = c(500, 1000, 1500, 2000),
                                labels = c("500 days", "1000 days", "1500 days", "2000 days"))) %>%
  arrange(landmark_time, est) %>%
  mutate(Order = row_number()) %>%
  {ggplot(., aes(x = est, y = Order)) +
      geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
      geom_point() +
      theme_bw() +
      xlab("Estimated importance") +
      ylab("Feature group") +
      xlim(c(0,0.3)) +
      scale_y_continuous(
        breaks = .$Order,
        labels = .$indx_name,
      ) + 
      facet_wrap(~landmark_time, dir = "v", strip.position = "right", scales = "free_y", ncol = 1) + 
      ggtitle("AUC variable importance relative to full feature vector")+
      theme(strip.background = element_blank(),
            strip.placement = "outside")
  }
p_auc
```

```{r, fig.width=8, fig.height=5}
# repeat the analysis for feature groups
for (i in 1:length(feature_group_names2)){
  indx_char <- feature_groups2[i]
  indx_name <- feature_group_names2[i]
  indx <- as.numeric(strsplit(indx_char, split = ",")[[1]])

  CV_reduced_preds <- CV_generate_reduced_predictions(time = time,
                                              event = event,
                                              X = X,
                                              landmark_times = landmark_times,
                                              folds = folds,
                                              sample_split = sample_split,
                                              indx = indx,
                                              full_preds_train = CV_full_preds_train,
                                              SL.library = SL.library)
  output <- vim_AUC(time = time,
                    event = event,
                    approx_times = approx_times,
                    landmark_times = landmark_times,
                    f_hat = lapply(CV_full_preds, function(x) 1-x),
                    fs_hat = lapply(CV_reduced_preds, function(x) 1-x),
                    S_hat = CV_S_preds,
                    G_hat = CV_G_preds,
                    folds = folds,
                    ss_folds = ss_folds,
                    sample_split = TRUE,
                    scale_est = TRUE)

  output$vim <- "AUC"
  output$indx <- rep(indx_char, nrow(output))
  output$indx_name <- rep(indx_name, nrow(output))
  if (!(i == 1)){
    pooled_output <- rbind(pooled_output, output)
  } else{
    pooled_output <- output
  }
}

p_auc <- pooled_output %>%
  mutate(landmark_time = factor(landmark_time,
                                levels = c(500, 1000, 1500, 2000),
                                labels = c("500 days", "1000 days", "1500 days", "2000 days"))) %>%
  arrange(landmark_time, est) %>%
  mutate(Order = row_number()) %>%
  {ggplot(., aes(x = est, y = Order)) +
      geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
      geom_point() +
      theme_bw() +
      xlab("Estimated importance") +
      ylab("Feature group") +
      xlim(c(0,0.5)) +
      scale_y_continuous(
        breaks = .$Order,
        labels = .$indx_name,
      ) +
      facet_wrap(~landmark_time, dir = "v", strip.position = "right", scales = "free_y", ncol = 1) +
      ggtitle("AUC variable importance relative to full feature vector (groups)")+
      theme(strip.background = element_blank(),
            strip.placement = "outside")
  }
p_auc
```

### Estimating variable importance relative to base model

Next, we consider the importance of each of the tumor-level features relative to a *baseline* set of person-level features. Here, the feature of interest is added to a baseline set of features, with importance measured by the resulting gain in predictiveness. 

For this analysis, the "full" oracle predictions include baseline features plus the feature of interest, and the "residual" oracle predictions include only baseline features. Note that both the full and residual oracle prediction functions for this analysis are estimated by regressing the conditional survival function estimates given **all** features on the relevant reduced feature set. As in the previous analysis, this step is necessary to account for censoring that may be informed by covariates, even those which are not included in the current set of predictors.

```{r, fig.width=8, fig.height=7}
# For importance relative to baseline features, the "reduced" model uses only person-level (baseline) features
# The "full" model uses baseline + feature of interest
# We wrote generate_reduced_predictions() to leave out the "indx" argument. Need to keep that in mind!
size_index <- paste0(c(size_index, person_index), collapse = ",")
nodes_index <- paste0(c(nodes_index, person_index), collapse = ",")
pgr_index <- paste0(c(pgr_index, person_index), collapse = ",")
er_index <- paste0(c(er_index, person_index), collapse = ",")
grade_index <- paste0(c(grade_index, person_index), collapse = ",")

feature_group_names <- c("size", "nodes", "prog.", "estro.", "grade")
feature_groups <- c(size_index, nodes_index,
                    pgr_index, er_index, grade_index)

CV_reduced_preds <- CV_generate_reduced_predictions(time = time,
                                                    event = event,
                                                    X = X,
                                                    landmark_times = landmark_times,
                                                    folds = folds,
                                                    sample_split = sample_split,
                                                    indx = as.numeric(strsplit(tum_index, split = ",")[[1]]),
                                                    full_preds_train = CV_full_preds_train,
                                                    SL.library = SL.library)

for (i in 1:length(feature_group_names)){
  indx_char <- feature_groups[i]
  indx_name <- feature_group_names[i]
  indx <- as.numeric(strsplit(indx_char, split = ",")[[1]])
  
  all_indx <- 1:ncol(X)
  # leave out features *not* in indx for this analysis
  indx <- all_indx[-which(all_indx %in% indx)]
  
  CV_full_preds <- CV_generate_reduced_predictions(time = time,
                                                   event = event,
                                                   X = X,
                                                   landmark_times = landmark_times,
                                                   folds = folds,
                                                   sample_split = sample_split,
                                                   indx = indx,
                                                   full_preds_train = CV_full_preds_train,
                                                   SL.library = SL.library)
  output <- vim_AUC(time = time,
                    event = event,
                    approx_times = approx_times,
                    landmark_times = landmark_times,
                    f_hat = lapply(CV_full_preds, function(x) 1-x),
                    fs_hat = lapply(CV_reduced_preds, function(x) 1-x),
                    S_hat = CV_S_preds,
                    G_hat = CV_G_preds,
                    folds = folds,
                    ss_folds = ss_folds,
                    sample_split = TRUE,
                    scale_est = TRUE)
  
  output$vim <- "AUC"
  output$indx <- rep(indx_char, nrow(output))
  output$indx_name <- rep(indx_name, nrow(output))
  if (!(i == 1)){
    pooled_output <- rbind(pooled_output, output)
  } else{
    pooled_output <- output
  }
}

p_auc <- pooled_output %>%
    mutate(landmark_time = factor(landmark_time,
                                levels = c(500, 1000, 1500, 2000),
                                labels = c("500 days", "1000 days", "1500 days", "2000 days"))) %>%
  arrange(landmark_time, est) %>%
  mutate(Order = row_number()) %>%
  {ggplot(., aes(x = est, y = Order)) +
      geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
      geom_point() +
      theme_bw() +
      xlab("Estimated importance") +
      ylab("Feature group") +
      xlim(c(0,0.4)) +
      scale_y_continuous(
        breaks = .$Order,
        labels = .$indx_name,
      ) + 
      facet_wrap(~landmark_time, dir = "v", strip.position = "right", scales = "free_y", ncol = 1) + 
      ggtitle("AUC variable importance relative to person-level features")+
      theme(strip.background = element_blank(),
            strip.placement = "outside")#,
  }
p_auc

```


## Adjustment variables

There may be covariates that are thought to influence both $T$ and $C$ but are not of scientific interest in terms of variable importance. (We may think of these covariates as analogous to confounders in a causal inference setting.) It is important to adjust for these variables in all analyses. In the `gbsg` analysis, for example, we may wish to adjust for person-level covariates age, menopausal status, and hormone treatment therapy, but to assess variable importance using only the predictiveness of tumor-level covariates. 

We use $r$ to denote the index set of adjustment variables, and again use $s$ to denote the index set of variables of interest. The importance of $X_s$ relative to $X_{-r}$ (i.e., the full covariate vector excluding adjustment variables) is given by $V(f_{0, r}, P_0) - V(f_{0, (r \cup s)}, P_0)$. 

As usual, there are many possible approaches to estimating $f_{0,r}$ and $f_{0,(r \cup s)}$, depending on their explicit form. In the case of AUC predictiveness, we can simply estimate the prediction models $f_{0,r}$ and $f_{0,(r \cup s)}$ by (1) estimating the full oracle prediction function $f_0$, (2) generating predictions for individuals in the training data, and then (3) regressing those predictions on the appropriate reduced covariate vector. 

Here, we analyze VIM relative to tumor-level covariates, while adjusting for person-level covariates. 

```{r, fig.width=8, fig.height=7}
size_index <- paste0(c(size_index, person_index), collapse = ",")
nodes_index <- paste0(c(nodes_index, person_index), collapse = ",")
pgr_index <- paste0(c(pgr_index, person_index), collapse = ",")
er_index <- paste0(c(er_index, person_index), collapse = ",")
grade_index <- paste0(c(grade_index, person_index), collapse = ",")

feature_group_names <- c("size", "nodes", "prog.", "estro.", "grade")
feature_groups <- c(size_index, nodes_index,
                    pgr_index, er_index, grade_index)

# in this analysis, "full" predictions are obtained by regressing the conditional survival
# function estimates given all features on the tumor-level features, i.e., leaving out
# person features
CV_full_preds <- CV_generate_reduced_predictions(time = time,
                                                    event = event,
                                                    X = X,
                                                    landmark_times = landmark_times,
                                                    folds = folds,
                                                    sample_split = sample_split,
                                                    indx = as.numeric(strsplit(person_index, split = ",")[[1]]),
                                                    full_preds_train = CV_full_preds_train,
                                                    SL.library = SL.library)

for (i in 1:length(feature_group_names)){
  indx_char <- feature_groups[i]
  indx_name <- feature_group_names[i]
  indx <- as.numeric(strsplit(indx_char, split = ",")[[1]])
  
  CV_reduced_preds <- CV_generate_reduced_predictions(time = time,
                                                   event = event,
                                                   X = X,
                                                   landmark_times = landmark_times,
                                                   folds = folds,
                                                   sample_split = sample_split,
                                                   indx = indx,
                                                   full_preds_train = CV_full_preds_train,
                                                   SL.library = SL.library)
  output <- vim_AUC(time = time,
                    event = event,
                    approx_times = approx_times,
                    landmark_times = landmark_times,
                    f_hat = lapply(CV_full_preds, function(x) 1-x),
                    fs_hat = lapply(CV_reduced_preds, function(x) 1-x),
                    S_hat = CV_S_preds,
                    G_hat = CV_G_preds,
                    folds = folds,
                    ss_folds = ss_folds,
                    sample_split = TRUE,
                    scale_est = TRUE)
  
  output$vim <- "AUC"
  output$indx <- rep(indx_char, nrow(output))
  output$indx_name <- rep(indx_name, nrow(output))
  if (!(i == 1)){
    pooled_output <- rbind(pooled_output, output)
  } else{
    pooled_output <- output
  }
}

p_auc <- pooled_output %>%
    mutate(landmark_time = factor(landmark_time,
                                levels = c(500, 1000, 1500, 2000),
                                labels = c("500 days", "1000 days", "1500 days", "2000 days"))) %>%
  arrange(landmark_time, est) %>%
  mutate(Order = row_number()) %>%
  {ggplot(., aes(x = est, y = Order)) +
      geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
      geom_point() +
      theme_bw() +
      xlab("Estimated importance") +
      ylab("Feature group") +
      xlim(c(0,0.4)) +
      scale_y_continuous(
        breaks = .$Order,
        labels = .$indx_name,
      ) + 
      facet_wrap(~landmark_time, dir = "v", strip.position = "right", scales = "free_y", ncol = 1) + 
      ggtitle("Adjusted AUC variable importance relative to all tumor-level features")+
      theme(strip.background = element_blank(),
            strip.placement = "outside")#,
  }
p_auc

```

## Doubly-robust estimation

The VIM estimation procedure implemented in `survML` is doubly-robust with respect to the conditional time-to-event survival function $S_0$ and conditional censoring survival function $G_0$: Roughly speaking, as long as one of these two nuisance functions is estimated well, the VIM estimator will tend in probability to the true population VIM as the sample size increases. 

In many cases, the oracle prediction functions $f_0$ and $f_{0,s}$ can themselves be estimated in a doubly-robust manner. For example, for AUC and Brier score VIMs evaluated at landmark time $\tau$, the oracle prediction function is simply $S_0(\tau \mid x)$. The pseudo-outcome approach of Rubin and van der Laan (2007) can be used to construct a doubly-robust estimator of a conditional survival function at a single time-point, at the computational cost of performing an additional regression step. Given initial estimates of $S_0$ and $G_0$, the following code constructs a doubly-robust estimate of $S_0(\tau \mid x)$ using Super Learner. 

```{r}
generate_DR_predictions <- function(time,
                                    event,
                                    X,
                                    X_holdout,
                                    landmark_times,
                                    approx_times,
                                    S_hat,
                                    G_hat,
                                    SL.library){

  DR_predictions <- matrix(NA, nrow = nrow(X_holdout), ncol = length(landmark_times))
  DR_predictions_train <- matrix(NA, nrow = nrow(X), ncol = length(landmark_times))
  for (i in 1:length(landmark_times)){
    tau <- landmark_times[i]
    Delta_t <- event * (time <= tau) + (time > tau)
    Y_t <- pmin(time, tau)

    # calculate m_0 at a specific time t
    calc_one <- function(t){
      if (approx_times[t] >= tau){
        m <-  rep(1, nrow(S_hat))
      } else{
        m <- S_hat[,which(approx_times == tau)]/S_hat[,t]
      }
      return(m)
    }
    # calculate m_0 over the whole grid
    ms <- matrix(unlist(lapply(1:length(approx_times), FUN = calc_one)),
                 nrow = length(time))

    m_Y <- apply(X = matrix(1:length(Y_t)), MARGIN = 1,
                 FUN = function(x) ms[x,which.min(abs(approx_times - Y_t[x]))])

    G_hat_Y <- apply(X = matrix(1:length(Y_t)), MARGIN = 1,
                     FUN = function(x) G_hat[x,which.min(abs(approx_times - Y_t[x]))])

    term1 <- Delta_t*(Y_t >= tau) / G_hat_Y

    term2 <- (1 - Delta_t) * m_Y / G_hat_Y

    int.vals <- t(sapply(1:length(time), function(j) {
      vals <- diff(1/G_hat[j,])* ms[j,-ncol(ms)]
      if(any(approx_times[-1] > Y_t[j])){
        vals[approx_times[-1] > Y_t[j]] <- 0
      }
      sum(vals)
    }))

    term3 <- t(int.vals)

    DR_pseudo_outcome <- term1 + term2 - term3

    SL_fit <- SuperLearner::SuperLearner(Y = DR_pseudo_outcome,
                                         X = X,
                                         family = stats::gaussian(),
                                         SL.library = SL.library,
                                         method = "method.NNLS",
                                         verbose = FALSE)
    SL_preds <- predict(SL_fit, newdata = X_holdout)$pred
    DR_predictions[,i] <- SL_preds
    SL_preds_train <- predict(SL_fit, newdata = X)$pred
    DR_predictions_train[,i] <- SL_preds_train

  }
  return(list(f_hat = DR_predictions, f_hat_train = DR_predictions_train))
}
```

This doubly-robust estimator of $f_0$ can then be regressed on the reduced covariate vector to give a doubly-robust estimator of $f_{0,s}$. These doubly-robust estimated oracle prediction functions can be used in the usual manner to estimate variable importance. Here, we repeat the original analysis using the doubly-robust pseudo-outcome approach. 

```{r, fig.width=8, fig.height=8}
# reset feature groups
age_index <- paste0(which(X_names == "age"))
meno_index <- paste0(which(X_names == "meno"))
size_index <- paste0(which(X_names == "size"))
nodes_index <- paste0(which(X_names == "nodes"))
pgr_index <- paste0(which(X_names == "pgr"))
er_index <- paste0(which(X_names == "er"))
hormon_index <- paste0(which(X_names == "hormon"))
grade_index <- paste0(which(X_names %in% c("tumgrad2", "tumgrad3")), collapse = ",")

feature_group_names <- c("age", "meno.", "size", "nodes",
                         "prog.", "estro.", "hormone", 
                         "grade")
feature_groups <- c(age_index, meno_index, size_index, nodes_index,
                    pgr_index, er_index, hormon_index, grade_index)

# generate nuisance estimates
V0_preds <- CV_generate_full_predictions(time = time,
                                         event = event,
                                         X = X,
                                         landmark_times = landmark_times,
                                         approx_times = approx_times,
                                         folds = folds,
                                         sample_split = TRUE,
                                         SL.library = SL.library,
                                         DR_pred = TRUE)

CV_full_preds <- V0_preds$CV_full_preds
CV_full_preds_train <- V0_preds$CV_full_preds_train
CV_S_preds <- V0_preds$CV_S_preds
CV_S_preds_train <- V0_preds$CV_S_preds_train
CV_G_preds <- V0_preds$CV_G_preds

# iterate over feature groups
for (i in 1:length(feature_group_names)){
  indx_char <- feature_groups[i]
  indx_name <- feature_group_names[i]
  indx <- as.numeric(strsplit(indx_char, split = ",")[[1]])

  # estimate residual oracle prediction function for this feature group
  CV_reduced_preds <- CV_generate_reduced_predictions(time = time,
                                              event = event,
                                              X = X,
                                              landmark_times = landmark_times,
                                              folds = folds,
                                              sample_split = sample_split,
                                              indx = indx,
                                              full_preds_train = CV_full_preds_train,
                                              SL.library = SL.library)
  # estimate VIM - note the oracle for AUC is the conditional cdf, not survival function, 
  # so need to take 1 - S(tau | x)
  output <- vim_AUC(time = time,
                    event = event,
                    approx_times = approx_times,
                    landmark_times = landmark_times,
                    f_hat = lapply(CV_full_preds, function(x) 1-x),
                    fs_hat = lapply(CV_reduced_preds, function(x) 1-x),
                    S_hat = CV_S_preds,
                    G_hat = CV_G_preds,
                    folds = folds,
                    ss_folds = ss_folds,
                    sample_split = TRUE,
                    scale_est = TRUE)
  
  output$vim <- "AUC"
  output$indx <- rep(indx_char, nrow(output))
  output$indx_name <- rep(indx_name, nrow(output))
  if (!(i == 1)){
    pooled_output <- rbind(pooled_output, output)
  } else{
    pooled_output <- output
  }
}

# plot results
p_auc <- pooled_output %>%
   mutate(landmark_time = factor(landmark_time,
                                levels = c(500, 1000, 1500, 2000),
                                labels = c("500 days", "1000 days", "1500 days", "2000 days"))) %>%
  arrange(landmark_time, est) %>%
  mutate(Order = row_number()) %>%
  {ggplot(., aes(x = est, y = Order)) +
      geom_errorbarh(aes(xmin = cil, xmax = ciu)) +
      geom_point() +
      theme_bw() +
      xlab("Estimated importance") +
      ylab("Feature group") +
      xlim(c(0,0.3)) +
      scale_y_continuous(
        breaks = .$Order,
        labels = .$indx_name,
      ) + 
      facet_wrap(~landmark_time, dir = "v", strip.position = "right", scales = "free_y", ncol = 1) + 
      ggtitle("AUC variable importance relative to all features (doubly-robust pseudo-outcome)")+
      theme(strip.background = element_blank(),
            strip.placement = "outside")
  }
p_auc
```

## Appendix

Some example predictiveness measures, along with the corresponding oracle prediction functions, are given below. 

* *AUC*:
    * $V(f, P_0) = P_0\{f(X_1) > f(X_2) \mid T_1 \leq \tau, T_2 > \tau\}$ 
    * $f_0(x) = 1 - S_0(\tau \mid x)$

* *Brier score*: 
    * $V(f, P_0) = E_{P_0}[\{f(X) - I(T > \tau)\}^2]$
    * $f_0(x) = S_0(\tau \mid x)$

* *Survival time MSE*: 
    * $V(f, P_0) = E_{P_0}[\{f(X) - (T \wedge \tau)\}^2]$
    * $f_0(x) = \int_0^\tau S_0(t \mid x)dt$

* *Proportion of explained variance*: 
    * $V(f, P_0) = \frac{E_{P_0}[\{f(X) - I(T > \tau)\}^2]}{var[I(T > \tau)]} $
    * $f_0(x) = S_0(\tau \mid x)$

* *Binary classification accuracy*: 
    * $V(f, P_0) = P_0\{f(X) = I(T > \tau)\}$
    * $f_0(x) = I\{S_0(\tau \mid x) > 0.5\}$

* *C-index*: 
    * $V(f, P_0) = P_0\{f(X_1) > f(X_2) \mid T_1 \leq T_2\}$
    * For the C-index, there is, to our knowledge, no closed form of the oracle prediction function; see the preprint for more details on our proposed procedure for direct numerical optimization. 



## References

The survival variable importance methodology is described in

Charles J. Wolock, Peter B. Gilbert, Noah Simon and Marco Carone. "Assessing variable importance in survival analysis using machine learning." [arXiv:2311.12726](https://arxiv.org/abs/2311.12726).

Other references: 

David R. Cox. ["Regression Models and Life-Tables."](https://doi.org/10.1111/j.2517-6161.1972.tb00899.x) *Journal of the Royal Statistical Society: Series B (Methodological)* (1972). 

Ted Westling, Alex Luedtke, Peter B. Gilbert and Marco Carone. ["Inference for treatment-specific survival curves using machine learning."](https://doi.org/10.1080/01621459.2023.2205060) *Journal of the American Statistical Association* (2023).

Charles J. Wolock, Peter B. Gilbert, Noah Simon and Marco Carone. ["A framework for leveraging machine learning tools to estimate personalized survival curves."](https://doi.org/10.1080/10618600.2024.2304070) *Journal of Computational and Graphical Statistics* (2024).

Mark J. van der Laan, Eric C. Polley and Alan E. Hubbard. ["Super learner"](https://doi.org/10.2202/1544-6115.1309). *Statistical Applications in Genetics and Molecular Biology* (2007). 

Daniel Rubin and Mark J. van der Laan. ["A doubly robust censoring unbiased transformation"](https://doi.org/10.2202/1557-4679.1052). *International Journal of Biostatistics* (2007). 
