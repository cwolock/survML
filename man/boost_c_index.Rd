% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boost_c_index.R
\name{boost_c_index}
\alias{boost_c_index}
\title{Gradient boosting for C-index}
\usage{
boost_c_index(
  time,
  event,
  X,
  newX,
  S_hat,
  G_hat,
  V,
  approx_times,
  tuning,
  produce_fit = TRUE,
  subsample_n,
  boosting_params
)
}
\arguments{
\item{time}{\code{n x 1} numeric vector of observed
follow-up times. If there is censoring, these are the minimum of the
event and censoring times.}

\item{event}{\code{n x 1} numeric vector of status indicators of
whether an event was observed.}

\item{X}{\code{n x p} data.frame of observed covariate values}

\item{newX}{\code{m x p} data.frame of new observed covariate
values at which to obtain \code{m} predictions for the estimated algorithm.
Must have the same names and structure as \code{X}.}

\item{S_hat}{\code{n x J2} matrix of conditional event time survival function estimates}

\item{G_hat}{\code{n x J2} matrix of conditional censoring time survival function estimates}

\item{V}{Number of cross-validation folds for selection of tuning parameters}

\item{approx_times}{Numeric vector of length J2 giving times at which to
approximate C-index integral. Note that the last time in \code{approx_times} is taken to be the restriction time (i.e., the maximum follow-up)
for comparison of pairs of individuals. Essentially, this time should be chosen such that the conditional survival function is identified at
this time for all covariate values \code{X} present in the data. Choosing the restriction time such that roughly 10\% of individuals remain at-risk
at that time has been shown to work reasonably well in simulations.}

\item{tuning}{Logical, whether or not to use cross-validation to select tuning parameters}

\item{produce_fit}{Logical, whether to produce a fitted prediction function using the selected optimal parameters.}

\item{subsample_n}{Number of samples to use for boosting procedure. Using a subsample of the full sample can greatly reduce runtime}

\item{boosting_params}{Named list of parameter values for the boosting procedure. Elements of this list include \code{mstop} (number of
boosting iterations), \code{nu} (learning rate), \code{sigma} (smoothness parameter for sigmoid approximation, with smaller meaning
less smoothing), and \code{learner} (base learner, can take values \code{"glm"}, \code{"gam"}, or \code{"tree"})}
}
\value{
Vector of predictions corresponding to \code{newX}
}
\description{
Using doubly-robust gradient boosting to generate estimates of the prediction function that maximizes the C-index
}
\examples{
# This is a small simulation example
set.seed(123)
n <- 250
X <- data.frame(X1 = rnorm(n), X2 = rbinom(n, size = 1, prob = 0.5))

T <- rexp(n, rate = exp(-2 + X[,1] - X[,2] + .5 *  X[,1] * X[,2]))
C <- rexp(n, exp(-2 -.5 * X[,1] - .25 * X[,2] + .5 * X[,1] * X[,2]))
C[C > 15] <- 15

time <- pmin(T, C)
event <- as.numeric(T <= C)

# Note that this a very small Super Learner library, for computational purposes.
SL.library <- c("SL.mean", "SL.glm")

# Note that we do not use times beyond the 90th percentile of observed follow-up times
approx_times <- c(0, unique(quantile(time, probs = seq(0, 0.9, by = 0.01))))

# estimate conditional survival functions at approx_times
fit <- stackG(time = time,
              event = event,
              X = X,
              newX = X,
              newtimes = approx_times,
              direction = "prospective",
              bin_size = 0.1,
              time_basis = "continuous",
              surv_form = "PI",
              learner = "SuperLearner",
              time_grid_approx = approx_times,
              SL_control = list(SL.library = SL.library,
                                V = 3))

# use boosting to estimate optimal (according to C-index) prediction function
boosted_preds <- boost_c_index(time = time,
                               event = event,
                               X = X,
                               newX = X,
                               approx_times = approx_times,
                               S_hat = fit$S_T_preds,
                               G_hat = fit$S_C_preds,
                               V = 3,
                               tuning = TRUE,
                               produce_fit = TRUE,
                               subsample_n = 200,
                               boosting_params = list(mstop = c(100, 200),
                                                      nu = 0.1,
                                                      sigma = 0.1,
                                                      learner = "glm"))
boosted_preds

}
